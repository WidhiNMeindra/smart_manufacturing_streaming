{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24d0a2fd-37e6-47ed-8d17-cb55909247da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c5541c7-80d9-4f14-a6ac-fb485fdd08be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigurasi logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cdec906-07a0-4537-b3c9-adc2ba2b469f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/07/15 13:36:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 13:36:12,114 - INFO - SparkSession berhasil dibuat.\n"
     ]
    }
   ],
   "source": [
    "# SEL 2: Membuat SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AnomalyModelTraining\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/opt/bitnami/spark/jars/postgresql-42.6.0.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"/opt/bitnami/spark/jars/postgresql-42.6.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "log.info(\"SparkSession berhasil dibuat dengan driver JDBC yang ditentukan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b801634-79ad-4e3c-ba61-9ebdc5b375ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 13:36:42,082 - INFO - Membaca data dari PostgreSQL...\n",
      "2025-07-15 13:36:45,222 - ERROR - Gagal membaca data dari database: An error occurred while calling o31.load.\n",
      ": java.sql.SQLException: No suitable driver\n",
      "\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:298)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:107)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:107)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_115/2839676406.py\", line 11, in <module>\n",
      "    .load() \\\n",
      "     ^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/sql/readwriter.py\", line 184, in load\n",
      "    return self._df(self._jreader.load())\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o31.load.\n",
      ": java.sql.SQLException: No suitable driver\n",
      "\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:298)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:107)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:107)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SEL 3: Membaca Data Latih dari PostgreSQL\n",
    "# Membaca data 'Normal' (label=0) langsung sebagai DataFrame Spark.\n",
    "log.info(\"Membaca data dari PostgreSQL...\")\n",
    "try:\n",
    "    df_normal = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:postgresql://postgres:5432/machine_db\") \\\n",
    "        .option(\"dbtable\", \"sensor_readings\") \\\n",
    "        .option(\"user\", \"user\") \\\n",
    "        .option(\"password\", \"password\") \\\n",
    "        .load() \\\n",
    "        .filter(col(\"label\") == 0)\n",
    "    \n",
    "    log.info(f\"Berhasil membaca {df_normal.count()} baris data 'Normal'.\")\n",
    "except Exception as e:\n",
    "    log.error(f\"Gagal membaca data dari database: {e}\", exc_info=True)\n",
    "    df_normal = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aa68778-c9bf-4ee8-97df-735a0d1f7f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEL 4: Persiapan Fitur dengan VectorAssembler\n",
    "# Spark MLlib memerlukan semua fitur input berada dalam satu kolom vektor.\n",
    "\n",
    "if df_normal:\n",
    "    feature_columns = ['vibration', 'acoustic', 'temperature', 'current', 'imf_1', 'imf_2', 'imf_3']\n",
    "    \n",
    "    # VectorAssembler menggabungkan beberapa kolom menjadi satu kolom vektor\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=feature_columns,\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    \n",
    "    # Terapkan assembler ke data kita\n",
    "    training_data = assembler.transform(df_normal).select(\"features\")\n",
    "    \n",
    "    log.info(\"Data telah diubah menjadi format vektor fitur.\")\n",
    "    training_data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73efbd52-55ec-4904-9e59-e75ffb74894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEL 5: Latih Model LocalOutlierFactor dari Spark MLlib\n",
    "if 'training_data' in locals():\n",
    "    log.info(\"Memulai pelatihan model K-Means Spark MLlib...\")\n",
    "    \n",
    "    # --- PERBAIKAN: Inisialisasi model K-Means ---\n",
    "    # Kita akan mengelompokkan data normal ke dalam 5 cluster.\n",
    "    kmeans = KMeans(featuresCol=\"features\", k=5, seed=1)\n",
    "    \n",
    "    # Latih model\n",
    "    model_kmeans = kmeans.fit(training_data)\n",
    "    \n",
    "    log.info(\"Pelatihan model K-Means selesai.\")\n",
    "    # Kita juga bisa menghitung ambang batas anomali di sini,\n",
    "    # tapi untuk sekarang kita simpan modelnya dulu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27dbbce-792f-426d-98c3-1ec513a82512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEL 6: Simpan Model Spark MLlib\n",
    "# Model Spark disimpan sebagai sebuah direktori, bukan satu file .pkl\n",
    "\n",
    "if 'model_anomaly_spark' in locals():\n",
    "    model_path = '/home/jovyan/work/spark_anomaly_model'\n",
    "    try:\n",
    "        # Hapus model lama jika ada\n",
    "        model_anomaly_spark.write().overwrite().save(model_path)\n",
    "        log.info(f\"Model Spark MLlib berhasil disimpan ke direktori: {model_path}\")\n",
    "    except Exception as e:\n",
    "        log.error(f\"Gagal menyimpan model: {e}\", exc_info=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
