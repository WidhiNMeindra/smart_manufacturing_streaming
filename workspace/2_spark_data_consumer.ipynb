{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c64eede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impor library yang diperlukan\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, expr, when, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ad6a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-51639060-f15a-4f63-92b4-f088e49f15e4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 1798ms :: artifacts dl 43ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-51639060-f15a-4f63-92b4-f088e49f15e4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/31ms)\n",
      "25/07/18 08:54:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession berhasil dibuat.\n"
     ]
    }
   ],
   "source": [
    "# Membuat SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PredictiveMaintenanceStreaming\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession berhasil dibuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ce6524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mendefinisikan Skema\n",
    "raw_schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"machine_id\", StringType(), True),\n",
    "    StructField(\"vibration\", DoubleType(), True),\n",
    "    StructField(\"acoustic\", DoubleType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"current\", DoubleType(), True),\n",
    "    StructField(\"IMF_1\", DoubleType(), True),\n",
    "    StructField(\"IMF_2\", DoubleType(), True),\n",
    "    StructField(\"IMF_3\", DoubleType(), True),\n",
    "    StructField(\"label\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b710b80-9197-4698-a278-13d80407bbe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Membaca dari topik Kafka\n",
    "raw_stream_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"raw_sensor_data\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d07d0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "038bf121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing dan Transformasi Data\n",
    "parsed_df = raw_stream_df \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), raw_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5920c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformasi utama\n",
    "# transformed_df = parsed_df \\\n",
    "#     .withColumn(\"event_timestamp\", to_timestamp(col(\"datetime\"))) \\\n",
    "#     .withColumn(\"status\",\n",
    "#         when((col(\"volt\") > 250) | (col(\"volt\") < 190), \"Voltage Anomaly\")\n",
    "#         .when(col(\"vibration\") > 70, \"High Vibration\")\n",
    "#         .otherwise(\"Normal\")\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cc6eeaf-7457-4e9f-87da-04c1eff4d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = parsed_df \\\n",
    "    .withColumn(\"event_timestamp\", to_timestamp(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"status\",\n",
    "        when(col(\"label\") == 1, \"Failure Detected\")\n",
    "        .otherwise(\"Normal\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90bcf659",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pilih kolom final\n",
    "final_df = transformed_df.select(\n",
    "    \"event_timestamp\", \"machine_id\", \"vibration\", \"acoustic\", \n",
    "    \"temperature\", \"current\", \"status\",\n",
    "    \"IMF_1\", \"IMF_2\", \"IMF_3\",\n",
    "    \"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b39ff858",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/18 08:55:10 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1468ca93-c8e4-443d-9fc9-b81c92ccb84e. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/07/18 08:55:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "# Memastikan transformasi benar sebelum menulis kembali ke Kafka.\n",
    "# Menulis ke konsol untuk debugging\n",
    "console_query = final_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01615e68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/18 08:55:12 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/07/18 08:55:12 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "25/07/18 08:55:12 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "25/07/18 08:55:12 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "25/07/18 08:55:12 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "25/07/18 08:55:12 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "25/07/18 08:55:20 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (172.18.0.8 executor 0): java.lang.NoSuchMethodError: 'boolean org.apache.spark.sql.catalyst.expressions.Cast$.apply$default$4()'\n",
      "\tat org.apache.spark.sql.kafka010.KafkaRowWriter.createProjection(KafkaWriteTask.scala:128)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaRowWriter.<init>(KafkaWriteTask.scala:76)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaDataWriter.<init>(KafkaDataWriter.scala:46)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaStreamWriterFactory.createWriter(KafkaStreamingWrite.scala:70)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.MicroBatchWriterFactory.createWriter(MicroBatchWrite.scala:51)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:459)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:514)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:411)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "25/07/18 08:55:20 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job\n",
      "25/07/18 08:55:20 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@290e9c54] is aborting.\n",
      "25/07/18 08:55:20 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@290e9c54] aborted.\n",
      "25/07/18 08:55:21 ERROR MicroBatchExecution: Query [id = aa3b93d3-efb4-4bad-9947-f75a8224c2ea, runId = e6e2e379-4e56-4915-b2a4-b487e7ff2b70] terminated with error\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 4) (172.18.0.8 executor 0): java.lang.NoSuchMethodError: 'boolean org.apache.spark.sql.catalyst.expressions.Cast$.apply$default$4()'\n",
      "\tat org.apache.spark.sql.kafka010.KafkaRowWriter.createProjection(KafkaWriteTask.scala:128)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaRowWriter.<init>(KafkaWriteTask.scala:76)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaDataWriter.<init>(KafkaDataWriter.scala:46)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaStreamWriterFactory.createWriter(KafkaStreamingWrite.scala:70)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.MicroBatchWriterFactory.createWriter(MicroBatchWrite.scala:51)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:459)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:514)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:411)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:408)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:382)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:330)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:341)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3418)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3418)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:738)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)\n",
      "Caused by: java.lang.NoSuchMethodError: 'boolean org.apache.spark.sql.catalyst.expressions.Cast$.apply$default$4()'\n",
      "\tat org.apache.spark.sql.kafka010.KafkaRowWriter.createProjection(KafkaWriteTask.scala:128)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaRowWriter.<init>(KafkaWriteTask.scala:76)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaDataWriter.<init>(KafkaDataWriter.scala:46)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaStreamWriterFactory.createWriter(KafkaStreamingWrite.scala:70)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.MicroBatchWriterFactory.createWriter(MicroBatchWrite.scala:51)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:459)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:514)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:411)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------------------+----------+---------+--------+-----------+-------+----------------+-----+------+-----+-----+\n",
      "|event_timestamp    |machine_id|vibration|acoustic|temperature|current|status          |IMF_1|IMF_2 |IMF_3|label|\n",
      "+-------------------+----------+---------+--------+-----------+-------+----------------+-----+------+-----+-----+\n",
      "|2024-07-01 08:00:00|M01       |0.822    |0.645   |66.85      |13.04  |Normal          |0.196|0.033 |0.0  |0    |\n",
      "|2024-07-01 08:01:00|M01       |1.398    |0.834   |76.2       |15.08  |Failure Detected|0.345|0.132 |0.001|1    |\n",
      "|2024-07-01 08:02:00|M01       |0.856    |0.59    |67.03      |12.3   |Normal          |0.187|0.017 |0.002|0    |\n",
      "|2024-07-01 08:03:00|M01       |0.793    |0.544   |65.04      |11.69  |Normal          |0.196|-0.06 |0.003|0    |\n",
      "|2024-07-01 08:04:00|M01       |1.279    |0.721   |78.19      |14.84  |Failure Detected|0.33 |-0.115|0.004|1    |\n",
      "|2024-07-01 08:05:00|M01       |0.782    |0.655   |61.95      |11.56  |Normal          |0.164|-0.081|0.005|0    |\n",
      "|2024-07-01 08:06:00|M01       |0.837    |0.564   |64.52      |10.98  |Normal          |0.238|-0.095|0.006|0    |\n",
      "|2024-07-01 08:07:00|M01       |0.725    |0.652   |62.87      |11.89  |Normal          |0.214|-0.076|0.007|0    |\n",
      "|2024-07-01 08:08:00|M01       |0.803    |0.702   |64.51      |11.19  |Normal          |0.145|-0.004|0.008|0    |\n",
      "|2024-07-01 08:09:00|M01       |0.779    |0.539   |61.3       |12.51  |Normal          |0.103|-0.025|0.009|0    |\n",
      "|2024-07-01 08:10:00|M01       |0.897    |0.627   |67.18      |11.18  |Normal          |0.093|0.097 |0.01 |0    |\n",
      "|2024-07-01 08:11:00|M01       |0.789    |0.603   |65.8       |11.27  |Normal          |0.086|-0.08 |0.011|0    |\n",
      "|2024-07-01 08:12:00|M01       |0.726    |0.575   |65.57      |12.85  |Normal          |0.079|-0.022|0.012|0    |\n",
      "|2024-07-01 08:13:00|M01       |0.826    |0.485   |66.41      |12.15  |Normal          |0.156|-0.005|0.013|0    |\n",
      "|2024-07-01 08:14:00|M01       |0.823    |0.582   |65.28      |12.31  |Normal          |0.217|0.05  |0.014|0    |\n",
      "|2024-07-01 08:15:00|M01       |0.882    |0.594   |62.76      |12.53  |Normal          |0.223|-0.059|0.015|0    |\n",
      "|2024-07-01 08:16:00|M01       |0.783    |0.631   |64.99      |12.56  |Normal          |0.164|-0.107|0.016|0    |\n",
      "|2024-07-01 08:17:00|M01       |0.793    |0.576   |66.74      |12.36  |Normal          |0.168|0.018 |0.017|0    |\n",
      "|2024-07-01 08:18:00|M01       |0.754    |0.6     |65.08      |12.44  |Normal          |0.077|-0.073|0.018|0    |\n",
      "|2024-07-01 08:19:00|M01       |0.917    |0.63    |65.19      |11.82  |Normal          |0.243|0.124 |0.019|0    |\n",
      "+-------------------+----------+---------+--------+-----------+-------+----------------+-----+------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Menulis ke topik Kafka yang sudah bersih\n",
    "kafka_output_df = final_df.select(expr(\"to_json(struct(*)) AS value\"))\n",
    "kafka_query = kafka_output_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"topic\", \"clean_sensor_data\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/spark_checkpoints/kafka_sensor_writer\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de23710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Untuk menghentikan semua query:\n",
    "console_query.stop()\n",
    "kafka_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e513de33-afb7-44fc-a4b8-0f879f708aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
