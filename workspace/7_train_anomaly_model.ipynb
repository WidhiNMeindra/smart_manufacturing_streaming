{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e90b268f-dc62-461c-89db-bf1b41ef34cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEL 1: Impor Library\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b7abf5a-73a2-483f-bc31-4f0d8cb1845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbdc8a74-be67-4524-8bcd-6d67ffa6c061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/18 05:42:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/18 05:42:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2025-07-18 05:42:22,450 - INFO - SparkSession berhasil dibuat.\n",
      "25/07/18 06:03:16 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "25/07/18 06:03:19 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:978)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# SEL 2: Membuat SparkSession dengan Konfigurasi Driver yang Benar\n",
    "# Ini akan terhubung ke klaster Spark yang SUDAH BERJALAN.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AnomalyModelTraining\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "log.info(\"SparkSession berhasil dibuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b49a89-b285-414d-9d88-a70ac2f615a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEL 3: Membaca Data (Hanya Data Normal) dari DB yang sudah berjalan\n",
    "log.info(\"Membaca data 'Normal' dari PostgreSQL...\")\n",
    "try:\n",
    "    df_normal = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:postgresql://postgres:5432/machine_db\") \\\n",
    "        .option(\"dbtable\", \"sensor_readings\") \\\n",
    "        .option(\"user\", \"user\") \\\n",
    "        .option(\"password\", \"password\") \\\n",
    "        .load() \\\n",
    "        .filter(col(\"label\") == 0)\n",
    "    log.info(f\"Berhasil membaca {df_normal.count()} baris data 'Normal'.\")\n",
    "except Exception as e:\n",
    "    log.error(\"Gagal membaca data dari database.\", exc_info=True)\n",
    "    df_normal = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d8a4c-01c2-45b7-9a84-72a27194ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEL 4: Persiapan Fitur\n",
    "if df_normal:\n",
    "    feature_columns = ['vibration', 'acoustic', 'temperature', 'current', 'imf_1', 'imf_2', 'imf_3']\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    training_data = assembler.transform(df_normal).select(\"features\")\n",
    "    log.info(\"Fitur telah digabungkan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f7eb9e-b9dc-4a1f-b619-9e6fef9fe9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEL 5: Latih Model K-Means\n",
    "if 'training_data' in locals():\n",
    "    log.info(\"Memulai pelatihan model K-Means...\")\n",
    "    kmeans = KMeans(featuresCol=\"features\", k=5, seed=1)\n",
    "    model_kmeans = kmeans.fit(training_data)\n",
    "    log.info(\"Pelatihan K-Means selesai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8132c5-346b-4240-9bfe-5e53669393eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEL 6: Simpan Model\n",
    "if 'model_kmeans' in locals():\n",
    "    model_path = '/home/jovyan/work/spark_kmeans_model'\n",
    "    try:\n",
    "        model_kmeans.write().overwrite().save(model_path)\n",
    "        log.info(f\"Model K-Means berhasil disimpan ke direktori: {model_path}\")\n",
    "    except Exception as e:\n",
    "        log.error(f\"Gagal menyimpan model: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ab8de-523a-4886-ae1d-f3db8206ce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEL 7: Hentikan SparkSession (PENTING!)\n",
    "# Kita menghentikan sesi ini agar tidak mengganggu pekerjaan streaming lain.\n",
    "spark.stop()\n",
    "log.info(\"SparkSession untuk pelatihan telah dihentikan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7443dba-84aa-42e7-952f-0c7a52df997b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
