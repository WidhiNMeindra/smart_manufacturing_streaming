{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "304d38f5-88aa-477b-ac22-3e9ca180d4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.3.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: pyspark==3.3.0 in /opt/conda/lib/python3.11/site-packages (3.3.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.11/site-packages (from pyspark==3.3.0) (0.10.9.5)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn joblib pyspark==3.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c64eede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, expr, when, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from typing import Iterator\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6b4a58a-c50e-4932-a35c-9444992de26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigurasi logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ca79143-d63c-4629-b48e-2930e0205100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fab1f4d6-030e-43ae-9232-631eab6efc5c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 1380ms :: artifacts dl 53ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-fab1f4d6-030e-43ae-9232-631eab6efc5c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/26ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/07/14 14:14:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2025-07-14 14:14:33,850 - INFO - SparkSession berhasil dibuat.\n"
     ]
    }
   ],
   "source": [
    "# Membuat spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RealtimeAnomalyDetection\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "log.info(\"SparkSession berhasil dibuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc118ec3-9f69-44cc-b747-31b9aa956159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 14:14:36,355 - INFO - Fungsi inferensi untuk .mapInPandas telah dibuat.\n"
     ]
    }
   ],
   "source": [
    "# SEL 4: Definisikan Fungsi Inferensi untuk .mapInPandas\n",
    "# Fungsi ini akan diterapkan pada setiap partisi data.\n",
    "\n",
    "# Definisikan urutan fitur yang sama persis seperti saat pelatihan\n",
    "feature_columns = ['vibration', 'acoustic', 'temperature', 'current', 'imf_1', 'imf_2', 'imf_3']\n",
    "model_path = '/home/jovyan/work/anomaly_detection_model.pkl'\n",
    "\n",
    "def predict_anomalies(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Fungsi ini memuat model sekali per partisi dan melakukan prediksi pada\n",
    "    seluruh potongan DataFrame, menghindari error serialisasi.\n",
    "    \"\"\"\n",
    "    # Muat model HANYA SEKALI per partisi/pekerja\n",
    "    model = joblib.load(model_path)\n",
    "    \n",
    "    # Loop melalui setiap potongan DataFrame dalam partisi\n",
    "    for pdf in iterator:\n",
    "        # Pilih kolom fitur dari DataFrame input\n",
    "        X = pdf[feature_columns]\n",
    "        # Buat kolom prediksi baru di dalam DataFrame Pandas\n",
    "        pdf['anomaly_prediction'] = model.predict(X)\n",
    "        # Kembalikan DataFrame yang sudah diperkaya dengan prediksi\n",
    "        yield pdf\n",
    "\n",
    "log.info(\"Fungsi inferensi untuk .mapInPandas telah dibuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ce6524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mendefinisikan Skema\n",
    "raw_schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"machine_id\", StringType(), True),\n",
    "    StructField(\"vibration\", DoubleType(), True),\n",
    "    StructField(\"acoustic\", DoubleType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"current\", DoubleType(), True),\n",
    "    StructField(\"IMF_1\", DoubleType(), True),\n",
    "    StructField(\"IMF_2\", DoubleType(), True),\n",
    "    StructField(\"IMF_3\", DoubleType(), True),\n",
    "    StructField(\"label\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b710b80-9197-4698-a278-13d80407bbe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 14:14:51,630 - INFO - Berhasil terhubung ke aliran data Kafka.\n"
     ]
    }
   ],
   "source": [
    "# Membaca dari topik Kafka\n",
    "raw_stream_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"raw_sensor_data\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "log.info(\"Berhasil terhubung ke aliran data Kafka.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d07d0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "033c70ff-d8bf-4e32-bf00-7342a56d5671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 14:14:58,177 - ERROR - Gagal membangun pipeline Spark.\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_8300/906051681.py\", line 18, in <module>\n",
      "    result_with_predictions_df = transformed_df.mapInPandas(predict_anomalies, schema=output_schema)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/sql/pandas/map_ops.py\", line 91, in mapInPandas\n",
      "    udf_column = udf(*[self[col] for col in self.columns])\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/sql/pandas/map_ops.py\", line 91, in <listcomp>\n",
      "    udf_column = udf(*[self[col] for col in self.columns])\n",
      "                       ~~~~^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/sql/dataframe.py\", line 1965, in __getitem__\n",
      "    jc = self._jdf.apply(item)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/sql/utils.py\", line 196, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: Cannot resolve column name \"anomaly_prediction\" among (timestamp, machine_id, vibration, acoustic, temperature, current, IMF_1, IMF_2, IMF_3, label, event_timestamp)\n"
     ]
    }
   ],
   "source": [
    "# SEL 6: Pipeline Transformasi dan Inferensi Real-time\n",
    "try:\n",
    "    # 1. Parsing data JSON dari Kafka\n",
    "    parsed_df = raw_stream_df \\\n",
    "        .select(from_json(col(\"value\").cast(\"string\"), raw_schema).alias(\"data\")) \\\n",
    "        .select(\"data.*\")\n",
    "\n",
    "    # 2. Transformasi dasar\n",
    "    transformed_df = parsed_df \\\n",
    "        .withColumn(\"event_timestamp\", to_timestamp(col(\"timestamp\"))) \\\n",
    "        .withColumn(\"machine_id\", col(\"machine_id\").cast(IntegerType()))\n",
    "\n",
    "    # --- PERBAIKAN: Terapkan fungsi inferensi menggunakan .mapInPandas ---\n",
    "    # Definisikan skema output setelah fungsi diterapkan\n",
    "    output_schema = transformed_df.schema.add(\"anomaly_prediction\", \"integer\")\n",
    "    \n",
    "    # Terapkan fungsi ke setiap partisi\n",
    "    result_with_predictions_df = transformed_df.mapInPandas(predict_anomalies, schema=output_schema)\n",
    "\n",
    "    # 3. Membuat kolom status akhir berdasarkan hasil deteksi anomali\n",
    "    final_df = result_with_predictions_df \\\n",
    "        .withColumn(\"status\",\n",
    "            when(col(\"anomaly_prediction\") == -1, \"Anomaly Detected\")\n",
    "            .otherwise(\"Normal\")\n",
    "        )\n",
    "\n",
    "    log.info(\"Pipeline transformasi dan inferensi telah dibangun.\")\n",
    "    final_df.printSchema()\n",
    "except Exception as e:\n",
    "    log.error(\"Gagal membangun pipeline Spark.\", exc_info=True)\n",
    "    final_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e513de33-afb7-44fc-a4b8-0f879f708aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEL 7: Menulis Hasil ke Konsol (Untuk Debugging)\n",
    "if final_df:\n",
    "    console_query = final_df \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .start()\n",
    "    \n",
    "    log.info(\"Query untuk menampilkan hasil ke konsol telah dimulai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebded923-d08f-4071-aa65-e00433ac5db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEL 8: Menulis Hasil ke Topik Kafka Baru (Untuk Downstream)\n",
    "if final_df:\n",
    "    kafka_output_df = final_df.select(expr(\"to_json(struct(*)) AS value\"))\n",
    "    \n",
    "    kafka_query = kafka_output_df \\\n",
    "        .writeStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "        .option(\"topic\", \"clean_data_with_anomaly\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/spark_checkpoints/anomaly_writer\") \\\n",
    "        .start()\n",
    "        \n",
    "    log.info(\"Query untuk menulis hasil ke Kafka telah dimulai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87011f6e-3111-4de6-976e-dd67d017bc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.3.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: pyspark==3.3.0 in /opt/conda/lib/python3.11/site-packages (3.3.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.11/site-packages (from pyspark==3.3.0) (0.10.9.5)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# SEL 1: Instalasi Library yang Diperlukan\n",
    "!pip install pandas scikit-learn joblib pyspark==3.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0488101-13fa-4795-8e91-7d1f353ce835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEL 2: Impor Library\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, expr, when, to_timestamp, struct, pandas_udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import logging\n",
    "from typing import Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d90f1913-e3de-4588-9003-c306cb810a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigurasi logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b4416e-918e-46b1-937a-fcabab3567bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-10aa1847-2852-4316-aaca-74b87a5b0542;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 1244ms :: artifacts dl 32ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-10aa1847-2852-4316-aaca-74b87a5b0542\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/31ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/07/14 14:26:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2025-07-14 14:27:00,474 - INFO - SparkSession berhasil dibuat.\n"
     ]
    }
   ],
   "source": [
    "# SEL 3: Membuat SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RealtimeAnomalyDetection\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "log.info(\"SparkSession berhasil dibuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a227e376-17cd-4580-b050-4326c740e3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 14:29:38,214 - INFO - Pandas UDF untuk deteksi anomali berhasil dibuat.\n"
     ]
    }
   ],
   "source": [
    "# SEL 4: Definisikan Fungsi Prediksi Menggunakan PANDAS UDF (Implementasi Andal)\n",
    "\n",
    "# Definisikan urutan fitur dan path model\n",
    "feature_columns = ['vibration', 'acoustic', 'temperature', 'current', 'imf_1', 'imf_2', 'imf_3']\n",
    "model_path = '/home/jovyan/work/anomaly_detection_model.pkl'\n",
    "\n",
    "# Gunakan decorator @pandas_udf untuk mendefinisikan fungsi\n",
    "@pandas_udf(IntegerType())\n",
    "def detect_anomaly_udf(*cols: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Fungsi ini menerima beberapa kolom Pandas, memuat model, \n",
    "    dan mengembalikan satu kolom (Series) hasil prediksi.\n",
    "    Ini adalah cara yang paling andal untuk inferensi model scikit-learn di Spark.\n",
    "    \"\"\"\n",
    "    # --- PERBAIKAN UTAMA: Muat model di dalam UDF ---\n",
    "    # Ini memastikan setiap worker Spark memiliki salinan modelnya sendiri\n",
    "    # dan menghindari semua masalah serialisasi (PicklingError).\n",
    "    model = joblib.load(model_path)\n",
    "    \n",
    "    # Menggabungkan semua kolom input menjadi satu DataFrame Pandas\n",
    "    X = pd.concat(cols, axis=1)\n",
    "    X.columns = feature_columns # Beri nama kolom yang benar\n",
    "    \n",
    "    # Lakukan prediksi\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    # Kembalikan hasilnya sebagai sebuah Series Pandas\n",
    "    return pd.Series(predictions)\n",
    "\n",
    "log.info(\"Pandas UDF untuk deteksi anomali berhasil dibuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b0b6ccc-4067-42b3-a975-38207fc11f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 14:29:53,718 - INFO - Berhasil terhubung ke aliran data Kafka.\n"
     ]
    }
   ],
   "source": [
    "# SEL 5: Definisikan Skema dan Baca Aliran Data dari Kafka\n",
    "raw_schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"machine_id\", StringType(), True),\n",
    "    StructField(\"vibration\", DoubleType(), True),\n",
    "    StructField(\"acoustic\", DoubleType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"current\", DoubleType(), True),\n",
    "    StructField(\"imf_1\", DoubleType(), True),\n",
    "    StructField(\"imf_2\", DoubleType(), True),\n",
    "    StructField(\"imf_3\", DoubleType(), True),\n",
    "    StructField(\"label\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "raw_stream_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"raw_sensor_data_v2\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "log.info(\"Berhasil terhubung ke aliran data Kafka.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c248c69-54b7-4760-8ad0-6cf0d9026e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 692, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 565, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 546, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 334, in _extract_code_globals\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 334, in <dictcomp>\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n",
      "2025-07-14 14:30:15,845 - ERROR - Gagal membangun pipeline Spark.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 692, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 565, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 546, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 334, in _extract_code_globals\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 334, in <dictcomp>\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_9772/2872844139.py\", line 12, in <module>\n",
      "    .withColumn(\"anomaly_prediction\", detect_anomaly_udf(*[col(c) for c in feature_columns]))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/sql/udf.py\", line 276, in wrapper\n",
      "    return self(*args)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/sql/udf.py\", line 249, in __call__\n",
      "    judf = self._judf\n",
      "           ^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/sql/udf.py\", line 215, in _judf\n",
      "    self._judf_placeholder = self._create_judf(self.func)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/sql/udf.py\", line 224, in _create_judf\n",
      "    wrapped_func = _wrap_function(sc, func, self.returnType)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/sql/udf.py\", line 50, in _wrap_function\n",
      "    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n",
      "                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/rdd.py\", line 3345, in _prepare_for_python_RDD\n",
      "    pickled_command = ser.dumps(command)\n",
      "                      ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/serializers.py\", line 468, in dumps\n",
      "    raise pickle.PicklingError(msg)\n",
      "_pickle.PicklingError: Could not serialize object: IndexError: tuple index out of range\n"
     ]
    }
   ],
   "source": [
    "# SEL 6: Pipeline Transformasi dan Inferensi Real-time\n",
    "try:\n",
    "    # 1. Parsing data JSON dari Kafka\n",
    "    parsed_df = raw_stream_df \\\n",
    "        .select(from_json(col(\"value\").cast(\"string\"), raw_schema).alias(\"data\")) \\\n",
    "        .select(\"data.*\")\n",
    "\n",
    "    # 2. Transformasi dasar dan inferensi model\n",
    "    transformed_df = parsed_df \\\n",
    "        .withColumn(\"event_timestamp\", to_timestamp(col(\"timestamp\"))) \\\n",
    "        .withColumn(\"machine_id\", col(\"machine_id\").cast(IntegerType())) \\\n",
    "        .withColumn(\"anomaly_prediction\", detect_anomaly_udf(*[col(c) for c in feature_columns]))\n",
    "\n",
    "    # 3. Membuat kolom status akhir berdasarkan hasil deteksi anomali\n",
    "    final_df = transformed_df \\\n",
    "        .withColumn(\"status\",\n",
    "            when(col(\"anomaly_prediction\") == -1, \"Anomaly Detected\")\n",
    "            .otherwise(\"Normal\")\n",
    "        )\n",
    "\n",
    "    log.info(\"Pipeline transformasi dan inferensi telah dibangun.\")\n",
    "    final_df.printSchema()\n",
    "except Exception as e:\n",
    "    log.error(\"Gagal membangun pipeline Spark.\", exc_info=True)\n",
    "    final_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba06350-1f0f-4124-b6ed-0d1bca0b2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEL 7: Menulis Hasil ke Konsol (Untuk Debugging)\n",
    "if final_df:\n",
    "    console_query = final_df \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .start()\n",
    "    \n",
    "    log.info(\"Query untuk menampilkan hasil ke konsol telah dimulai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf4212e-13e2-4383-9b9a-78343b60bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEL 8: Menulis Hasil ke Topik Kafka Baru (Untuk Downstream)\n",
    "if final_df:\n",
    "    kafka_output_df = final_df.select(expr(\"to_json(struct(*)) AS value\"))\n",
    "    \n",
    "    kafka_query = kafka_output_df \\\n",
    "        .writeStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "        .option(\"topic\", \"clean_data_with_anomaly\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/spark_checkpoints/anomaly_writer\") \\\n",
    "        .start()\n",
    "        \n",
    "    log.info(\"Query untuk menulis hasil ke Kafka telah dimulai.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
